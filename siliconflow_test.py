# -*- coding: utf-8 -*-

'''
ç¡…åŸºæµåŠ¨å¹³å°APIæµ‹è¯•å·¥å…·

åŠŸèƒ½è¯´æ˜ï¼š
- æ”¯æŒç¡…åŸºæµåŠ¨APIçš„æµå¼å“åº”è°ƒç”¨
- å®æ—¶æ˜¾ç¤ºAIå“åº”å†…å®¹
- æä¾›å®Œæ•´çš„æ€§èƒ½ç»Ÿè®¡æŒ‡æ ‡
- ç¾åŒ–çš„è¡¨æ ¼è¾“å‡ºæ ¼å¼
- æ€§èƒ½æ•°æ®å¯è§†åŒ–å±•ç¤º

è¿è¡Œå‘½ä»¤ï¼š
python siliconflow_test.py

é…ç½®å‚æ•°ï¼š
â­ APIå¯†é’¥ï¼šéœ€åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®SILICONFLOW_API_KEY
â­ APIåœ°å€ï¼šå¯åœ¨base_urlä¸­ä¿®æ”¹APIæ¥å£åœ°å€
â­ æ¨¡å‹é€‰æ‹©ï¼šå¯åœ¨modelå‚æ•°ä¸­æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹
â­ æµå¼å“åº”ï¼šstreamå‚æ•°æ§åˆ¶æ˜¯å¦ä½¿ç”¨æµå¼å“åº”

æ€§èƒ½æŒ‡æ ‡è¯´æ˜ï¼š
- ç½‘ç»œå»¶è¿Ÿï¼šä»å‘é€è¯·æ±‚åˆ°å»ºç«‹è¿æ¥çš„æ—¶é—´
- é¦–tokenå“åº”ï¼šä»å‘é€è¯·æ±‚åˆ°æ”¶åˆ°ç¬¬ä¸€ä¸ªtokençš„æ—¶é—´
- å®é™…è¾“å‡ºè€—æ—¶ï¼šå»é™¤ç½‘ç»œå»¶è¿Ÿå’Œé¦–tokenå“åº”åçš„çº¯è¾“å‡ºæ—¶é—´
- Tokenç»Ÿè®¡ï¼šä½¿ç”¨UTF-8ç¼–ç å­—èŠ‚é•¿åº¦è®¡ç®—
'''

import sys
import time
import os
import json
import requests
from dotenv import load_dotenv
from tabulate import tabulate

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è·å–é…ç½®å‚æ•°
api_key = os.getenv('SILICONFLOW_API_KEY')
base_url = os.getenv('SILICONFLOW_BASE_URL')
model_id = os.getenv('SILICONFLOW_MODEL_ID')

# æ‰“å°é…ç½®ä¿¡æ¯
print(f"ä½¿ç”¨APIåœ°å€: {base_url}")
print(f"ä½¿ç”¨æ¨¡å‹: {model_id}")
print(f"APIå¯†é’¥: {api_key[:5]}...{api_key[-5:] if api_key else 'None'}")

# æµ‹è¯•æ¶ˆæ¯
# =============================================
# ğŸ”§ æµ‹è¯•æ¶ˆæ¯é…ç½® (å¸¸ç”¨ä¿®æ”¹éƒ¨åˆ†)
# =============================================
messages = [
    {"role": "user", "content": "100.9å’Œ100.11è°å¤§ï¼Ÿ"}
]

# è®¡ç®—è¾“å…¥token
def calculate_input_tokens(messages):
    total_tokens = 0
    for message in messages:
        total_tokens += len(message['content'].encode('utf-8'))
    return total_tokens

# è®¡ç®—è¾“å‡ºtoken
def calculate_output_tokens(content):
    return len(content.encode('utf-8'))

# è¾“å‡ºæµå¼å†…å®¹
def write_stream_content(text):
    sys.stdout.write(text)
    sys.stdout.flush()

# ç”Ÿæˆæ€§èƒ½ç»Ÿè®¡è¡¨æ ¼
def generate_performance_table(metrics):
    headers = ['æŒ‡æ ‡', 'æ•°å€¼', 'å•ä½']
    rows = [
        ['ç½‘ç»œå»¶è¿Ÿ', f"{metrics['network_latency']:.2f}", 'ç§’'],
        ['é¦–tokenå“åº”', f"{metrics['first_token_time']:.2f}", 'ç§’'],
        ['å®é™…è¾“å‡ºè€—æ—¶', f"{metrics['output_time']:.2f}", 'ç§’'],
        ['æ€»è€—æ—¶', f"{metrics['total_time']:.2f}", 'ç§’']
    ]
    return tabulate(rows, headers=headers, tablefmt='grid')

# ç”ŸæˆTokenç»Ÿè®¡è¡¨æ ¼
def generate_token_table(metrics):
    headers = ['Tokenç±»å‹', 'æ•°é‡', 'å¤‡æ³¨']
    rows = [
        ['è¾“å…¥Token', metrics['input_tokens'], '-'],
        ['è¾“å‡ºToken', metrics['output_tokens'], '-'],
        ['æ€»Token', metrics['total_tokens'], '-'],
        ['è¾“å‡ºé€Ÿç‡', f"{metrics['output_speed']:.2f}", 'token/s']
    ]
    return tabulate(rows, headers=headers, tablefmt='grid')

print("æ­£åœ¨å‘é€APIè¯·æ±‚...")

try:
    # åˆå§‹åŒ–æ€§èƒ½æŒ‡æ ‡
    metrics = {
        'network_latency': 0,
        'first_token_time': 0,
        'output_time': 0,
        'total_time': 0,
        'input_tokens': 0,
        'output_tokens': 0,
        'total_tokens': 0,
        'output_speed': 0
    }
    
    # APIè¯·æ±‚URL - ç¡…åŸºæµåŠ¨APIçš„æ ‡å‡†æ ¼å¼
    url = f"{base_url}/v1/chat/completions"
    
    # æ‰“å°å®Œæ•´çš„è¯·æ±‚URL
    print(f"å®Œæ•´è¯·æ±‚URL: {url}")
    
    # è¯·æ±‚ä½“ - ä½¿ç”¨ä¸å•ç‹¬æµ‹è¯•ç›¸åŒçš„æ ¼å¼
    payload = {
        "model": model_id,
        "messages": messages,
        "stream": True,
        "max_tokens": 512,
        "stop": ["null"],
        "temperature": 0.7,
        "top_p": 0.7,
        "top_k": 50,
        "frequency_penalty": 0.5,
        "n": 1,
        "response_format": {"type": "text"}
    }
    
    # è¯·æ±‚å¤´
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    print(f"è¯·æ±‚æ¨¡å‹: {model_id}")
    print(f"è¯·æ±‚ä½“: {json.dumps(payload, ensure_ascii=False)}")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    first_token_time = None
    network_latency = None
    content = ""
    
    # å‘é€æµå¼è¯·æ±‚
    response = requests.post(url, json=payload, headers=headers, stream=True)
    
    # è·å–é¦–æ¬¡ç½‘ç»œè¿æ¥æ—¶é—´
    network_latency = time.time() - start_time
    
    # æ£€æŸ¥å“åº”çŠ¶æ€
    if response.status_code != 200:
        raise Exception(f"APIè¯·æ±‚å¤±è´¥: HTTP {response.status_code} - {response.text}")
    
    print("\nğŸ” å¼€å§‹æ¥æ”¶å“åº”æµï¼š")
    
    # å¤„ç†æµå¼å“åº”
    for line in response.iter_lines():
        if line:
            # è®°å½•é¦–ä¸ªtokençš„æ—¶é—´
            if first_token_time is None:
                first_token_time = time.time()
                print("é¦–ä¸ªtokenå“åº”æ—¶é—´ï¼š{:.2f}ç§’".format(first_token_time - start_time))
            
            # è§£æSSEæ ¼å¼çš„æ•°æ®
            line_text = line.decode('utf-8')
            if line_text.startswith('data: '):
                json_str = line_text[6:]  # å»æ‰'data: 'å‰ç¼€
                if json_str.strip() == '[DONE]':
                    break
                
                try:
                    delta = json.loads(json_str)
                    if 'choices' in delta and len(delta['choices']) > 0:
                        if 'delta' in delta['choices'][0] and 'content' in delta['choices'][0]['delta']:
                            chunk = delta['choices'][0]['delta']['content']
                            content += chunk
                            write_stream_content(chunk)
                except json.JSONDecodeError:
                    continue
    
    # è®¡ç®—ç»“æŸæ—¶é—´å’Œæ€»è€—æ—¶
    end_time = time.time()
    total_time = end_time - start_time
    
    # è®¡ç®—å®é™…è¾“å‡ºè€—æ—¶ï¼ˆæ€»æ—¶é—´ - é¦–ä¸ªtokenæ—¶é—´ - ç½‘ç»œå»¶è¿Ÿï¼‰
    output_time = total_time - (first_token_time - start_time) if first_token_time else 0
    
    # è®¡ç®—tokenä½¿ç”¨æƒ…å†µ
    input_tokens = calculate_input_tokens(messages)
    output_tokens = calculate_output_tokens(content)
    output_speed = output_tokens / output_time if output_time > 0 else 0
    
    # å‡†å¤‡æ€§èƒ½æŒ‡æ ‡æ•°æ®
    performance_metrics = {
        'network_latency': network_latency or 0,
        'first_token_time': (first_token_time - start_time) if first_token_time else 0,
        'output_time': output_time,
        'total_time': total_time
    }
    
    # å‡†å¤‡Tokenç»Ÿè®¡æ•°æ®
    token_metrics = {
        'input_tokens': input_tokens,
        'output_tokens': output_tokens,
        'total_tokens': input_tokens + output_tokens,
        'output_speed': output_speed
    }
    
    # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
    print("\n\nğŸ“Š æ€§èƒ½ç»Ÿè®¡ï¼š")
    print(generate_performance_table(performance_metrics))
    
    print("\nğŸ“ Tokenç»Ÿè®¡ï¼š")
    print(generate_token_table(token_metrics))

except Exception as e:
    print("\nâŒ å‘ç”Ÿé”™è¯¯ï¼š{}".format(str(e)))
    
    # å³ä½¿å‘ç”Ÿé”™è¯¯ä¹Ÿè®¡ç®—å·²è·å¾—çš„æŒ‡æ ‡
    end_time = time.time()
    total_time = end_time - start_time
    metrics = {
        'network_latency': 0,
        'first_token_time': 0,
        'output_time': 0,
        'total_time': total_time,
        'input_tokens': calculate_input_tokens(messages),
        'output_tokens': 0,
        'total_tokens': calculate_input_tokens(messages),
        'output_speed': 0
    }
    
    # æ˜¾ç¤ºå·²è·å¾—çš„æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡ï¼š")
    print(generate_performance_table(metrics))
    print("\nğŸ“ Tokenç»Ÿè®¡ï¼š")
    print(generate_token_table(metrics))