# -*- coding: utf-8 -*-

'''
è…¾è®¯äº‘APIæµ‹è¯•å·¥å…·

åŠŸèƒ½è¯´æ˜ï¼š
- æ”¯æŒè…¾è®¯äº‘APIçš„æµå¼å“åº”è°ƒç”¨
- å®æ—¶æ˜¾ç¤ºAIå“åº”å†…å®¹
- æä¾›å®Œæ•´çš„æ€§èƒ½ç»Ÿè®¡æŒ‡æ ‡

è¿è¡Œå‘½ä»¤ï¼š
python tencent_test.py

é…ç½®å‚æ•°ï¼š
â­ APIå¯†é’¥ï¼šéœ€åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®TENCENT_API_KEY
â­ APIåœ°å€ï¼šå¯åœ¨base_urlä¸­ä¿®æ”¹APIæ¥å£åœ°å€
â­ æ¨¡å‹é€‰æ‹©ï¼šå¯åœ¨modelå‚æ•°ä¸­æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹
â­ æµå¼å“åº”ï¼šstreamå‚æ•°æ§åˆ¶æ˜¯å¦ä½¿ç”¨æµå¼å“åº”
'''

import sys
import time
from openai import OpenAI
from config import config

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = OpenAI(
    api_key=config.tencent['api_key'],
    base_url=config.tencent['base_url']
)

# æµ‹è¯•æ¶ˆæ¯
# =============================================
# ğŸ”§ æµ‹è¯•æ¶ˆæ¯é…ç½® (å¸¸ç”¨ä¿®æ”¹éƒ¨åˆ†)
# =============================================
messages = [
    {"role": "user", "content": "100.9å’Œ100.11è°å¤§ï¼Ÿ"}
]
# ä½¿ç”¨é…ç½®å‚æ•°
model = config.tencent['model']
stream = config.tencent['stream']

# è®¡ç®—è¾“å…¥token
def calculate_input_tokens(messages):
    total_tokens = 0
    for message in messages:
        total_tokens += len(message['content'].encode('utf-8'))
    return total_tokens

# è®¡ç®—è¾“å‡ºtoken
def calculate_output_tokens(content):
    return len(content.encode('utf-8'))

# è¾“å‡ºæµå¼å†…å®¹
def write_stream_content(text):
    sys.stdout.write(text)
    sys.stdout.flush()

print("æ­£åœ¨å‘é€APIè¯·æ±‚...")

try:
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    first_token_time = None
    network_latency = None
    content = ""
    reasoning_content = ""
    
    # å‘é€æµå¼è¯·æ±‚
    response = client.chat.completions.create(
        model=model,  # ä½¿ç”¨é…ç½®çš„æ¨¡å‹
        messages=messages,
        stream=stream  # ä½¿ç”¨é…ç½®çš„æµå¼å“åº”è®¾ç½®
    )
    
    # è·å–é¦–æ¬¡ç½‘ç»œè¿æ¥æ—¶é—´
    network_latency = time.time() - start_time
    
    print("\nğŸ” å¼€å§‹æ¥æ”¶å“åº”æµï¼š")
    
    # å¤„ç†æµå¼å“åº”
    for chunk in response:
        # è®°å½•é¦–ä¸ªtokençš„æ—¶é—´
        if first_token_time is None:
            first_token_time = time.time()
            print("é¦–ä¸ªtokenå“åº”æ—¶é—´ï¼š{:.2f}ç§’".format(first_token_time - start_time))
        
        # å¤„ç†æ¨ç†å†…å®¹
        if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
            reasoning_content += chunk.choices[0].delta.reasoning_content
            write_stream_content(chunk.choices[0].delta.reasoning_content)
        # å¤„ç†æ™®é€šå†…å®¹
        elif hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
            content += chunk.choices[0].delta.content
            write_stream_content(chunk.choices[0].delta.content)
    
    # è®¡ç®—ç»“æŸæ—¶é—´å’Œæ€»è€—æ—¶
    end_time = time.time()
    total_time = end_time - start_time
    
    # è®¡ç®—å®é™…è¾“å‡ºè€—æ—¶ï¼ˆæ€»æ—¶é—´ - é¦–ä¸ªtokenæ—¶é—´ - ç½‘ç»œå»¶è¿Ÿï¼‰
    output_time = total_time - (first_token_time - start_time) - network_latency
    
    # è®¡ç®—tokenä½¿ç”¨æƒ…å†µ
    input_tokens = calculate_input_tokens(messages)
    output_tokens = calculate_output_tokens(content)
    
    # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
    print("\n\nğŸ“Š æ€§èƒ½ç»Ÿè®¡ï¼š")
    print("ç½‘ç»œå»¶è¿Ÿï¼š{:.2f}ç§’".format(network_latency))
    print("é¦–ä¸ªtokenå“åº”ï¼š{:.2f}ç§’".format(first_token_time - start_time))
    print("å®é™…è¾“å‡ºè€—æ—¶ï¼š{:.2f}ç§’".format(output_time))
    print("æ€»è€—æ—¶ï¼š{:.2f}ç§’".format(total_time))
    print("\nğŸ“ Tokenç»Ÿè®¡ï¼š")
    print("è¾“å…¥Tokenï¼š{}".format(input_tokens))
    print("è¾“å‡ºTokenï¼š{}".format(output_tokens))
    if output_time > 0:
        output_speed = output_tokens / output_time
        print("è¾“å‡ºé€Ÿåº¦ï¼š{:.2f} token/s (è®¡ç®—æ–¹å¼ï¼š{} / {:.2f})".format(output_speed, output_tokens, output_time))
    print("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
    print("æ€»æ¶ˆè€—Tokenï¼š{}".format(input_tokens + output_tokens))

except Exception as e:
    print("\nâŒ å‘ç”Ÿé”™è¯¯ï¼š{}".format(str(e)))
    exit(1)